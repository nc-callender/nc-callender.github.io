# Project 3: Automation and Predictive Modelling 

# Project 3: Automation and Predictive Modelling 

## Project Outline
This project focused on prediction whether a patient was diabetic using other health indicators.  It used a dataset available on Kaggle [link]( https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset).  The data was split into five education levels (No School or Elementary School Only, Some High School, Completed High School, Some College or technical School, and Completed College.)
For each education level, EDA (Exploratory Data Analysis) was performed.  The data was split into training and test sets.  The training set was used to train a variety of models (logistic regression, LASSO logistic regression, classification tree, random forest, ridge, and elastic net.  The training utilized five-fold cross validation with Log Loss as the performance metric.  These optimized models were then used to make predictions on the test set.  The predicted results were compared to observed results and used to calculate Log Loss and Accuracy for the prediction.  These metrics were used to compare the models and winners declared.  Lastly, the accuracy of the models were compared to the accuracy of a simple “Predict the Most Popular” model.  (Log loss was not calculated for that model since it would be infinity for all the data points that are not the most popular.

## Links
Click [here]( https://github.com/nc-callender/ST-558-Project-3) to visit the repo for this project.
Click [here]( https://nc-callender.github.io/ST-558-Project-3/) to visit the landing page for this project.

## What To Do Differently
When I went to start the modelling section of the project, I tried to troubleshoot the problems within the working program.  So any changes I made in the data caused errors to crop up in the code already in place for EDA. which had to be fixed before I could find out if it fixed the problem in the modelling section-which it did not.  Eventually I realized I would be better off going to the `iris`  dataset and making one change at a time to it so it would look more like my data.  Quickly,  I learned that the caret:train really did not want the response variables to be 0 and 1, and really, really did not want the response variable to have a hyphen.  Next time, I will switch from troubleshoot mode to learn using a small built-in dataset much faster.  (This really enhanced my appreciation for GitHub, since I could just pull the version of the file stored prior to having mucking it up while trying to troubleshoot.)

## Most Difficult Part
I ran into two stumbling blocks in this model.  One involved my understanding of factors and the other involved trying to use a package for something that could be easily programmed. 
Prior to this project, I though of factor levels as the data and factor labels as sort of an alternate description that could be substituted when printing. (Essentially I thought they were like labels for variables in SAS.)  When I went to train my models, I expected that it was  using the levels rather than the labels.  Alas, it was not! And one of my labels (Non-diabetic) had a hyphen.  And the caret::train function did not like the hyphen.  Now I understand, for factors in R, the levels are the input and they are converted to the labels. 
During testing of the models, I wanted to calculate Log Loss and found a package function (MLmetrics::LogLoss) to do that.  It kept giving me errors regarding the nature of its input.  The error messages did not make sense to me,  and I spent a lot of time trying to troubleshoot it.  Eventually, I realized I was spending a lot of time doing this when I could just program the calculation myself using commands from the Tidyverse in a couple of minutes.  A painful reminder that sometimes trying to avoid a task is way more work than doing the task. 
## Big Takeaways
The biggest takeaway is the impact than having an unbalanced dataset can have on the prediction algorithm.  Specifically models will come to be dominated by the most popular of the response choices. In this case, for each education level, the best performing model exhibited no more than a 2% improvement relative to a “pick the most popular” model.  
